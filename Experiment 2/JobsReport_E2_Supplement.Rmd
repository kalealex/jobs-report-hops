---
title: "JobsReport_E2_Results"
author: ""
date: "6/22/2018"
output: html_document
---

```{r setup, include=FALSE}
library(dotwhisker)
library(broom)
library(lme4)
library(lmerTest)
library(tidyverse)
library(knitr)
library(gmodels)
library(lsmeans)
knitr::opts_chunk$set(echo = TRUE)
```

#Experiment 2 Results

##Data

We start by loading participant-level data containing estimates of psychometric function (PF) parameters per participant. These parameter estimates were computed in Matlab using a combination of custom analysis scripts and a library of PF fitting functions from Geoffrey Boynton. PF fitting code is available in our repo. Custom scripts are available upon request but are not included in supplemental materials becasue they contain non-anonymized MTurk WorkerIDs.

A power analysis based on the data from experiment 1 suggested we would need 50 participants *per visualization condition* to detect *between-subjects* differences in just-noticable differences (JNDs) for regular HOPs, fast HOPs, and line ensembles with 80% power. This power analysis assumes that we are trying detect effects of similar magnitude to the effect in experiment 1. Following our [preregistered](https://osf.io/975us/register/5771ca429ad5a1020de2872e) analysis plan, we iteratively collected data and excluded PF fits based on poor fit quality and poor performance. Overall, we recruited 62 participants. Six of these participants were excluded per our preregistered exclusion criteria. Data for the sample of 150 participants used for the statistical inferences presented in the paper are in the files "E2-AnonymousStats.csv" and "E2-AnonymousRawData.csv".

We'll focus mostly on the estimates of PF parameters in the file "E2-AnonymousStats.csv" in order to reproduce the analyses presented in the paper.

```{r}
statsDf = read.csv("E2-AnonymousStats.csv")
```
 
The variables in this data sets are as follows.

1. Subject: MTurk workerIDs 
  + These are anonymized identifiers (not actual worker IDs) in order to maintain privacy.
  + Each participant has one row in the data frame; there are 150 participants.
2. Visualization: the visualization condition under which data were collected
  + Coding: c = line ensembles; h = HOPs (2.5 hz; 400 ms per sample); hf = fast HOPs (10 hz; 100 ms per sample)
  + Each participant completed two blocks of trials under one of three visualization conditions (between-subjects).
3. StartCond: the visualization condition on which a worker started
  + Coding of conditions is identical to the Visualization variable.
  + This variable is redundant with the Visualization variable, a leftover in our analysis pipeline from experiment 1.
4. Threshold: the JND fit to each observer’s data under each visualization condition
  + JDNs are in units of the absolute value of the log likelihood ratio that a stimulus was produced by the no growth vs the growth trend.
  + The JND measures the level of evidence at which the participant is expected to answer with their mean accuracy.
  + The JND is the point on the x-axis which corresponds to the mean value of the psychometric function (PF) on the y-axis.
5. Spread: the standard deviation of the psychometric function (PF) fit to an observer’s data under each visualization condition
  + The Spread parameter of the PF shares the same units as the JND.
  + This is a measure of the width of the PF.
  + This parameter estimate is inversely proportional to the incline of the PF at its inflection point (aka the JND).
  + PF spread represents the noise in the observer’s perception of the evidence presented in a stimulus.
6. ConfidenceFitness: a mixing parameter describing the degree to which reported confidence values are predicted by a statistical formulation of confidence vs randomly sampled confidence values
  + Units range from 0 (totally random confidence reporting) to 1 (confidence reporting is in sync with statistical confidence).
7. CompletionTime: the number of milliseconds the participant spent completing the trials used to fit each psychometric function
  + This is the entire time participants had the webpage open between the beginning of the task and their answer on the last trial, so this should *not* be considered a controlled measure of time spent attending to the task. This time does not include time spent reading the instructions.

We also load in the raw trial-level response data for reference in our analysis.

```{r}
rawDf = read.csv("E2-AnonymousRawData.csv")
```

##Linear Models

We use linear models for statistical inference. Details can be found in our [preregistered](https://osf.io/gw4cj/register/5771ca429ad5a1020de2872e) analysis plan.

```{r}
# linear models for each outcome variable
tMdl <- lm(Threshold ~ Visualization, data = statsDf)
sMdl <- lm(Spread ~ Visualization, data = statsDf)
cMdl <- lm(ConfidenceFitness ~ Visualization, data = statsDf)
```

##Results Per Measure

###JNDs

A summary of our linear model on JND estimates.

```{r}
summary(tMdl)
confint(tMdl)
```

We plot the regression coeficients as dots with error bars. This shows estimated effect sizes (differences between the two HOPs conditions and the line ensembles condition) with 95% CIs.

```{r echo=FALSE}
dwplot(tMdl, show_intercept = TRUE) %>% 
  relabel_predictors(c(Visualizationh = "Visualization HOPs",
                       Visualizationhf = "Visualization Fast HOPs")) +
  theme_bw() + xlab("Coefficient Estimate") + ylab("") +
  geom_vline(xintercept = 0, colour = "grey60", linetype = 2) +
  ggtitle("Coefficients for JNDs") +
  theme(plot.title = element_text(face="bold"),
      legend.position="none",
      panel.grid.major = element_blank(), 
      panel.grid.minor = element_blank(),
      panel.background = element_blank()) +
  scale_colour_grey(start = .3)
```

We see smaller JNDs in the regular HOPs condition relative to the control condition, but we do not see the same magnitude of effect for fast HOPs.

Pairwise comparisons adjusted for multiple comparisons (i.e., Tukey's HSD) confirm our interpretation of the linear model above. The only reliable difference in JNDs is between the regular HOPs and line ensembles visualization conditions. The JNDs for fast HOPs are intermediate between regular HOPs and line ensembles but not reliably different.

```{r}
tAov <- aov(Threshold ~ Visualization, data = statsDf)
TukeyHSD(tAov)
```

It is noteworthy that this effect is driven by a small group of observers who performed much worse in the line ensembles and fast HOPs conditions than observers in the regular HOPs condition. In light of this data, it seems that our effect of visualization on JNDs is best characterized as a difference of consistency between observers in the ability to use these visualizations to do the task rather than a difference in performance among all observers.

```{r echo=FALSE}
ggplot(data = statsDf, mapping = aes(x = Visualization, y = Threshold, color = Visualization)) +
  geom_boxplot(mapping = aes(x = Visualization, y = Threshold)) + 
  geom_jitter(alpha=0.6) + 
  ggtitle(label = "JNDs Per Visualization Condition")
```

Reviewers asked whether this subgroup of participants with poor performance (larger JNDs) on the line ensembles condition is accounted for by the time spent completing the task. To check this, we compare the model of JNDs presented in the paper to a similar model including the time spent to complete the trials used to fit each PF as a predictor.

```{r}
# convert completion time from milliseconds to minutes
statsDf$CompletionTime <- statsDf$CompletionTime / 1000 / 60
# specify the model with completion time as a predictor
tMdl2 <- lm(Threshold ~ Visualization + CompletionTime, data = statsDf)
```
```{r echo=FALSE}
dwplot(list(tMdl, tMdl2), show_intercept = TRUE) %>% 
  relabel_predictors(c(Visualizationh = "Visualization HOPs",
                       Visualizationhf = "Visualization Fast HOPs",
                       CompletionTime = "Time to Complete Trials")) +
  theme_bw() + xlab("Coefficient Estimate") + ylab("") +
  geom_vline(xintercept = 0, colour = "grey60", linetype = 2) +
  ggtitle("Coefficients for JNDs") +
  theme(plot.title = element_text(face="bold"),
      legend.position=c(0.06, 0.02), legend.justification=c(0, 0),
      legend.title = element_text(size=8),
      legend.background = element_rect(color="gray90"),
      legend.spacing = unit(-4, "pt"),
      legend.key.size = unit(10, "pt"),
      panel.grid.major = element_blank(), 
      panel.grid.minor = element_blank(),
      panel.background = element_blank()) +
  scale_colour_grey(start = .3, end = .7,
      name = "Model Specification",
      # breaks = c(0, 1),
      labels = c("Without Time", "With Time"))
```

We can see that adding completion time to the model doesn't impact the model coefficients.

###Spreads

A summary of our linear model on PF spread estimates.

```{r}
summary(sMdl)
confint(sMdl)
```

Again, we plot estimated effect sizes as dots with error bars representing 95% CIs.

```{r echo=FALSE}
dwplot(sMdl, show_intercept = TRUE) %>% 
  relabel_predictors(c(Visualizationh = "Visualization HOPs",                       
                       Visualizationhf = "Visualization Fast HOPs")) +
  theme_bw() + xlab("Coefficient Estimate") + ylab("") +
  geom_vline(xintercept = 0, colour = "grey60", linetype = 2) +
  ggtitle("Coefficients for PF Spreads") +
  theme(plot.title = element_text(face="bold"),
      legend.position="none",
      panel.grid.major = element_blank(), 
      panel.grid.minor = element_blank(),
      panel.background = element_blank()) +
  scale_colour_grey(start = .3)
```

We see no effect of visualization condition on noise in the perception of evidence, as measured by PF spreads.

###Confidence Fitness

A summary of our linear model on confidence fitness estimates.

```{r}
summary(cMdl)
confint(cMdl)
```

We visualize estimated effect sizes as dots with error bars representing 95% CIs. We are modeling confidnece fitness, a mixing parameter from 0 to 1 describing the degree to which reported confidence corresponds to a statistical formulation of confidence.

```{r echo=FALSE}
dwplot(cMdl, show_intercept = TRUE) %>% 
  relabel_predictors(c(Visualizationh = "Visualization HOPs",                       
                       Visualizationhf = "Visualization Fast HOPs")) +
  theme_bw() + xlab("Coefficient Estimate") + ylab("") +
  geom_vline(xintercept = 0, colour = "grey60", linetype = 2) +
  ggtitle("Coefficients for Confidence Fitness") +
  theme(plot.title = element_text(face="bold"),
      legend.position="none",
      panel.grid.major = element_blank(), 
      panel.grid.minor = element_blank(),
      panel.background = element_blank()) +
  scale_colour_grey(start = .3)
```

We see no effect of visualization condition on confidence fitness. Similarly we saw a null result for confidence fitness in the first experiment.

Now we look at a mixed effects linear model raw confidence data. In the first experiment, this analysis was exploratory, but we [preregistered](https://osf.io/gw4cj/register/5771ca429ad5a1020de2872e) this model as a secondary analysis for experiment 2.

```{r}
# Log likelihood ratio (Ratio) is stored in the raw data with signs (negative vs positive) indicating the data-generating model for the stimulus, where positive log ratios indicate no growth and negative log ratios indicate a growth trend.
# We need to take the absolute value of this log likelihood ratio in order to model confidence as a function of evidence regardless of the data generating model, as we do in the paper.
rawDf$Evidence <- abs(rawDf$Ratio)
# specify linear model
rawConfMdl <-lmer(Confidence ~ Evidence * Correct + Visualization + (1|WorkerID), data = rawDf)
summary(rawConfMdl)
confint(rawConfMdl)
```
```{r echo=FALSE}
dwplot(rawConfMdl, show_intercept = TRUE) %>% 
  relabel_predictors(c(Evidence = "Level of Evidence",                       
                       CorrectTRUE = "Correct Response",
                       Visualizationh = "Visualization HOPs",                       
                       Visualizationhf = "Visualization Fast HOPs",
                       "Evidence:CorrectTRUE" = "Evidence and Correct Response Interaction")) +
  theme_bw() + xlab("Coefficient Estimate") + ylab("") +
  geom_vline(xintercept = 0, colour = "grey60", linetype = 2) +
  ggtitle("Coefficients for Reported Confidence") +
  theme(plot.title = element_text(face="bold"),
      legend.position="none",
      panel.grid.major = element_blank(), 
      panel.grid.minor = element_blank(),
      panel.background = element_blank()) +
  scale_colour_grey(start = .3)
```

Here, we see main effects for evidence, correctness, and the fast HOPs visualization condition as well as a significant interaction between evidence and correctness. The increase in reported confidence on correct trials and the interaction between stimulus intensity and correctness were expected based on the findings of Sanders et al. (2016), who created the confidence fitness model. Confidence goes up with stimulus intensity for trials where the participant was correct, but confidence goes down with increasing stimulus intensity on trials where participants were wrong. We can see this trend by looking at our raw confidence reports, although the plot is crowded.

```{r echo=FALSE}
ggplot(rawDf, aes(x=Evidence, y=Confidence, color=Correct)) + geom_point() + ggtitle(label="Confidence Reports by Level of Evidence and Correctness")
```

Sanders et al. (2016) found that the expected confidence generated by their model predicted this interaction between correctness and level of evidence in a stimulus. As they interpretted it, this behavior comports with the statistical formulation of confidence used in our model. Let's check for this predictive behavior in our expected confidence estimations.

```{r echo=FALSE}
# some values for estimated confidence fall below 50
rawDf %>% filter(ExpectedConfidence > 50) -> rawExpectedConfInRange
# try to recreate figure 2 from Sanders et al. for each participant
ggplot(rawExpectedConfInRange, aes(x=Evidence, color=Correct)) + stat_summary(aes(y=Confidence),fun.y=mean, geom="point", size=1) + stat_summary(aes(y=ExpectedConfidence),fun.y=mean, geom="line", size=0.5) + facet_wrap(~ WorkerID) + theme(strip.background = element_blank(), strip.text.x = element_blank())
```

For many observers, reported confidence (dots) covers a wider range of the y-axis than expected confidence (lines). We've traced the origin of this difference in variability to the Monte Carlo simulation. For subjects with narrow PFs, the amount of noise added to evidence on each trial to generate simulated percepts is small. This means that the *simulated observer* only gets trials wrong where the evidence is really close to 0 (indicating that the stimulus conveys minimal information to disambiguate the underlying trend). A concequence of the *simulated observer* perceiving most stimuli correctly is that the model predicts values of confidence
$$Pr(correct \mid perceivedEvidence)$$
which are constant and high across most values of perceived evidence. In other words, low noise in simulated percepts leads to low variability in predicted confidence. This might explain the lack of good predictive behavior for subjects with small PF spreads. 

See the file "JobsReport_ConfidenceFitness_Supplement.Rmd" for a detailed explanation of the confidence fitness algorithm and additional remarks on the model's strengths and limitations.

Next we consider the main effect of the fast HOPs visualization on confidence reporting. It is important to acknowledge that this is a small effect on average, no greater than -6 units on our confidence scale. Let's visualize our confidence reporting data and try to see this effect.

```{r echo=FALSE}
ggplot(rawDf, aes(x=Visualization, y=Confidence, color=Visualization)) +  
  geom_violin(trim=FALSE, adjust=1.5) + 
  # geom_jitter(pch="-", size=4) +
  stat_summary(fun.y=mean, geom="point", size=3) +
  ggtitle(label="Reported Confidence by Visualization")
```

We can see that participants are more confident on average in the line ensemble condition than in either of the HOPs conditions, although only the difference between fast HOPs and line ensembles achieves statistical significance. Interestingly, this effect is in the opposite direction from our E1, where HOPs were associated with higher reported confidence on average than error bars.

##Additional Analysis of Response Bias

In our task participants must interpret which of two data generating scenarios ('growth' or 'no growth' in the job market) is more likely to have produced a given sample of jobs numbers. Overall, are participants more likely to answer 'growth' than 'no growth', or vice-versa? We can see that the frequency of each answer is approximately equal as a proportion of the number of trials, which is what we would expect given that the experiment is designed to have an equal number of trials where 'growth' and 'no growth' are the correct answer.

```{r}
# proportion of 'no growth' responses in the raw data
sum(rawDf$Response=="steady") / length(rawDf$Response)
# proportion of 'growth' responses in the raw data
sum(rawDf$Response=="increase") / length(rawDf$Response)
```

We also want to know what makes our participants more likely to answer 'growth' vs 'no growth'. To address this we conduct and exploratory analysis on responses for each trial. We use logistic mixed effects regression estimate response ('growth' or 'no growth') as a function of the fixed effects of visualization condition, log likelihood ratio (signed evidence), and whether or not a participant’s answer was correct. We also model a random effect of participant. We do not model an interaction between log likelihood ratio and correctness because the estimation procedure does not converge due to some combinations of these predictors where there are very few observations.

```{r}
# logistic regression of response bias
bMdl <- glmer( Response ~ Ratio + Correct + Visualization + (1 | WorkerID), data = rawDf, family = binomial)
summary(bMdl)
confint(bMdl)
```

In order to understand this model, we need to know how variables are coded in the model:

* Responses of 'growth' are coded as 0 and responses of 'no growth' are coded as 1. This means that positive coefficiencts indicate that a predictor makes a response of 'no growth' more likely, and negative coefficiencts indicate that a predictor makes a response of 'growth' more likely.
* Ratio in this model is signed so that negative values indicate evidence for the 'growth' scenario and positive values indicate evidence for the 'no growth' scenario. Throughout the study, we use the absolute value of this log likelihood ratio as our metric of evidence for perceptual decision making, but here we do *not* take the absolute value.
* The variables Correct and Visualization are coded as they are in the other models presented here.

Let's plot the coefficients for this logistic mixed effects regression to see how these different predictors and their interactions bias responses.

```{r echo=FALSE}
dwplot(bMdl, show_intercept = TRUE) %>% 
  relabel_predictors(c(Ratio = "Log Likelihood Ratio",
                       CorrectTRUE = "Correct Response",
                       Visualizationh = "Visualization Regular HOPs",
                       Visualizationhf = "Visualization Fast HOPs")) +
  theme_bw() + xlab("Coefficient Estimate") + ylab("") +
  geom_vline(xintercept = 0, colour = "grey60", linetype = 2) +
  ggtitle("Coefficients for Response Bias") +
  theme(plot.title = element_text(face="bold"),
      legend.position = "none",
      panel.grid.major = element_blank(), 
      panel.grid.minor = element_blank(),
      panel.background = element_blank()) +
  scale_colour_grey(start = .3)
```

We can see that there is a reliable effect of log likelihood ratio (level of evidence for the data generating scenario), and a boarderline effect of correctness. Let's try to understand these effects one at a time.

How people respond depending on the log likelihood ratio (level of evidence in the stimulus) is perhaps the easiest to understand. Here we plot response frequency against log likelihood ratio.

```{r echo=FALSE}
# bin ratio by partitions of 2 units to get bars
rawDf$RatioBinned <- cut(rawDf$Ratio, breaks=seq(from=-10, to=10, by=2))
ggplot(rawDf, aes(x=RatioBinned,fill=Response)) + geom_bar(stat="count",position=position_dodge())
```

We can see that participants' responses roughly follow the pattern we would expect. Correct responses become more frequent as log likelihood ratio moves away from 0 (the point of no evidence). As the logistic regression coefficient for log likelihood ratio indicates, participants respond 'no growth' more frequenty as log likelihood ratio increases, representing greater evidence that 'no growth' is the data-generating scenario.

Next, we try to understand the effect of correctness on responses. To illustrate this, we plot frequencies for each response on trials when users are correct vs when they are incorrect.

```{r echo=FALSE}
ggplot(data = rawDf, mapping = aes(x = Correct, fill = Response)) + 
  geom_bar(stat="count",position=position_dodge()) +
  ggtitle(label = "Responses by Correctness")
```

Note that it is a little backwards to think of correctness as *predicting* a certain response since responses are graded against the log likelihood ratio of each stimulus in order to generate the correctness variable. However, this makes more sense if we think about the effect of correctness in terms of conditional probability based on our data. Knowing that a response is correct means that, in the observed data, a participant was slightly more likely to have responded 'growth'. Similarly, knowing that a response is incorrect means that, in the observed data, a participant was slightly more likely to have responded 'no growth'. Since these differences are small, we doubt that they impact the findings we present in the paper.
