---
title: "One Model To Rule Them All"
author: "Alex Kale"
date: "9/24/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# library(rethinking)
library(rstan)
```

## Experiment 2

### Load Raw Data

These data were collected in a two-alternative forced choice experiement where participants judged stimuli at varying levels of evidence and were either correct or incorrect on each trial. In experiment 2, each participant was assigned to one of three possible uncertainty visualization conditions: regular HOPs, fast HOPs, or line ensembles.

```{r}
# e1df = read.csv("E1-AnonymousRawData-InferenceSample.csv")
e2df = read.csv("E2-AnonymousRawData.csv")
```

### Preprocessing

Reformat the raw data from the experiment as a list, changing types to integers as needed and taking the absolute value of ratio (i.e., stimulus intensity). This step will enable Stan to correctly process the data.

```{r}
# format data for model
e2data <- list(
  N = length(e2df$Ratio),
  S = length(unique(e2df$WorkerID)),
  correct = as.integer(e2df$Correct),
  intensity = abs(e2df$Ratio),
  hops = as.integer(e2df$Visualization=="h"),
  fast = as.integer(e2df$Visualization=="hf"),
  lines = as.integer(e2df$Visualization=="c"),
  subject = as.integer(as.factor(e2df$WorkerID))
)
```

### Model Specification

We model the probability of a correct response from 0.5 (guessing) to 1 - lapse rate (the maximum estimated performance for each participant) as a function of stimulus intensity using a _scaled and shifted cumulative normal distribution_. This is called the *psychometric function*. We define the psychometric function. Note that the parameters of the psychometric function are defined in this function:

* threshold: mean of the cumulative Gaussian in stimulus intensity units,
* spread: standard deviation (sd) of the cumulative Gaussian in stimulus intensity units,
* guess rate: the lower bound of the cumulative probability density which is .5 for a two alternative forced choice task with equally likely alternatives
* lapse rate: the rate of error for an individual due to lapses of attention which are independent of the stimulus

The following chunk of code specifies the psychometric function. We will use the psychometric function to estimate accuracy as a function of other variables of interest given the fit of our statistical model.

```{r}
psychometric <- function(intensity,m,sd,guess,lapse){
  # handle arguments
  if(missing(guess)) {
    guess <- .5
  }
  if( missing(lapse)) {
    lapse <- 0
  }
  intensity <- as.matrix(intensity)

  # set upper and lower bounds
  lowerBound <- guess
  upperBound <- 1 - lapse

  # calculate z score for each level of intensity
  r <- dim(intensity)[1]
  c <- dim(intensity)[2]
  z <- (intensity - m * matrix(1, r, c)) / sd

  # scale cumulative probability density between upper and lower bound
  p <- lowerBound + pnorm(z / sqrt(2)) * (upperBound - lowerBound)
  return(p)
}
```

Our statistical model is a variation on *Bayesian hierarchical probit regression*.

We use a _scaled and shifted probit as our link function_ in a multi-level binomial regression. Here is the specification of the statistical model. Note the following features of the model:

* We are modelling correctness as a binomial distributed outcome with the probability of being correct as a transformed linear model of ratio and visualization condition.
* We use our scaled probit function (defined above) as the link function for our linear model.
* The threshold, spread, and lapse rate parameters of the model are estimated for each subject while guess rate is assumed to be .5 based on the design of the experiment.
* We estimate the effect of each visualization condition via dummy coding and the effect of stimulus intensity (ratio).

The point of this exercise is that all parameters, both at the subject level (psychometric function parameters) and the group level (effects of visualization condition and stimulus intensity), are all estimated from raw data within one unified model.

```{r}
# model specification in Stan
stanCode <- 
'data {
  int<lower=0> N; // number of obs
  int<lower=0> S; // number of subjects
  int<lower=0,upper=1> correct[N]; // correctness of judgments on each trial
  real<lower=0> intensity[N]; // stimulus intensity on each trial
  int<lower=0,upper=1> hops[N]; // dummy code for regular speed HOPs
  int<lower=0,upper=1> fast[N]; // dummy code for fast HOPs
  int<lower=0,upper=1> lines[N]; // dummy code for line ensembles
  int<lower=0> subject[N]; //subject index for each observation
}
parameters {
  real<lower=0,upper=15> threshold[S]; // estimate threshold, spread, and lapse parameters of the PF for each subject
  real<lower=0> spread[S];
  real<lower=0,upper=0.06> lapse[S];
  real a; // model coefficients
  real bi;
  real bh;
  real bf;
  real bl; 
  real<lower=0,upper=1> p[N]; // probability of being correct on each trial
}
model {
  threshold ~ uniform(0,15); // threshold, spread, and lapse parameters of the PF
  spread[subject] ~ cauchy(0,1);
  lapse[subject] ~ uniform(0,.06);
  a ~ normal(0,10); //weak priors for model coefficients
  bi ~ normal(0,10);
  bh ~ normal(0,10);
  bf ~ normal(0,10);
  bl ~ normal(0,10);
  for (i in 1:N) {
    // scaled and shifted probit link function specific to each subject
    (threshold[subject[i]] + spread[subject[i]]*(sqrt(2)*inv_Phi(p[i] - 0.5)/(0.5 - lapse[subject[i]]))) <- a + bi*intensity[i] + bh*hops[i] + bf*fast[i] + bl*lines[i];
  }
  correct ~ binomial(1,p);
}'
```

Next we fit the model to our data using Stan.

```{r}
e2m <- stan(
  model_code = stanCode,
  data = e2data,
  chains = 4,
  cores = 2,
  warmup = 500,
  iter = 1500
)
```


<!-- ### Check Model Perfomance -->

<!-- This kind of Bayesian model samples from a Markov Chain Monte Claro algoritm to estimate each parameter in our model. Here we can see whether the four separate simulations we ran above converged (a sign of reliable estimates) and whether the any the samples from any simulation contain autocorrelation (a sign that out simulation got stuck at a given location in parameter space, and we should run the model fit again and possibly change our model specification or priors). For me this model seemed to run just fine. -->

<!-- ```{r} -->
<!-- plot(m) -->
<!-- ``` -->





<!-- ### OLD Preprocessing -->

<!-- ```{r} -->
<!-- # format data for model -->
<!-- e2df$correct <- as.integer(e2df$Correct) -->
<!-- e2df$intensity <- abs(e2df$Ratio) -->
<!-- e2df$hops <- as.integer(e2df$Visualization=="h") -->
<!-- e2df$fast <- as.integer(e2df$Visualization=="hf") -->
<!-- e2df$lines <- as.integer(e2df$Visualization=="c") -->

<!-- # make sure stan doesn't read our data wrong (e.g., in case there are missing subject ids) -->
<!-- e2df$subject <- as.integer(as.factor(e2df$WorkerID)) -->
<!-- ``` -->

<!-- ### OLD Model Specification -->

<!-- Here we model the probability of a correct response from 0.5 (guessing) to 1 - lapse rate (the maximum estimated performance for each participant) as a function of stimulus intensity using a cumulative Gaussian distribution. This is called the psychometric function. We define the psychometric function and its inverse (i.e., a scaled probit) below. Note that the parameters of the psychometric function are defined in these functions: -->

<!-- * threshold: mean of the cumulative Gaussian in stimulus intensity units, -->
<!-- * spread: standard deviation (sd) of the cumulative Gaussian in stimulus intensity units, -->
<!-- * guess rate: the lower bound of the cumulative probability density which is .5 for a two alternative forced choice task with equally likely alternatives -->
<!-- * lapse rate: the rate of error for an individual due to lapses of attention which are independent of the stimulus -->

<!-- ```{r} -->
<!-- # define the psychometric function (PF) and inverse as a rescaled probit, adding parameters for mean, sd, guess rate, and lapse rate of the PF -->
<!-- scaledProbit <- function(p,m,sd,guess,lapse){ -->
<!--   # handle arguments -->
<!--   if(missing(guess)) { -->
<!--     guess <- .5 -->
<!--   } -->
<!--   if( missing(lapse)) { -->
<!--     lapse <- 0 -->
<!--   } -->
<!--   p <- as.matrix(p) -->

<!--   # set upper and lower bounds -->
<!--   lowerBound <- guess -->
<!--   upperBound <- 1 - lapse -->

<!--   # calculate z scores for each value of p -->
<!--   z <- sqrt(2) * qnorm((p - lowerBound) / (upperBound - lowerBound)) -->

<!--   # convert z scores to units of intensity -->
<!--   intensity <- z * sd + m -->
<!--   # place lower bound for stimulus intensity at zero -->
<!--   intensity[is.infinite(intensity)] <- 0 -->
<!--   return(intensity) -->
<!-- } -->
<!-- psychometric <- function(intensity,m,sd,guess,lapse){ -->
<!--   # handle arguments -->
<!--   if(missing(guess)) { -->
<!--     guess <- .5 -->
<!--   } -->
<!--   if( missing(lapse)) { -->
<!--     lapse <- 0 -->
<!--   } -->
<!--   intensity <- as.matrix(intensity) -->

<!--   # set upper and lower bounds -->
<!--   lowerBound <- guess -->
<!--   upperBound <- 1 - lapse -->

<!--   # calculate z score for each level of intensity -->
<!--   r <- dim(intensity)[1] -->
<!--   c <- dim(intensity)[2] -->
<!--   z <- (intensity - m * matrix(1, r, c)) / sd -->

<!--   # scale cumulative probability density between upper and lower bound -->
<!--   p <- lowerBound + pnorm(z / sqrt(2)) * (upperBound - lowerBound) -->
<!--   return(p) -->
<!-- } -->
<!-- ``` -->


<!-- Next, we use the scaled probit as our link function in a binomial regression. Here is the specification of the statistical model. Note the following features of the model: -->

<!-- * We are modelling correctness as a binomial distributed outcome with the probability of being correct as a transformed linear model of ratio and visualization condition. -->
<!-- * We use our scaled probit function (defined above) as the link function for our linear model. -->
<!-- * The threshold, spread, and lapse rate parameters of the model are estimated for each subject while guess rate is assumed to be .5 based on the design of the experiment. -->
<!-- * We estimate the effect of each visualization condition via dummy coding and the effect of stimulus intensity (ratio). -->

<!-- The point of this exercise is that all parameters, both at the subject level (psychometric function parameters) and the group level (effects of visualization condition and stimulus intensity), are all estimated from raw data within one unified model. -->


<!-- ```{r} -->
<!-- # from Richard McElreath's Statistical Rethinking (http://xcelab.net/rm/statistical-rethinking/) -->
<!-- e2m <- map2stan( -->
<!--   alist( -->
<!--     correct ~ dbinom(1,p), # binomial distribution of correctness on each trial -->
<!--     scaledProbit(p,threshold[subject],spread[subject],0.5,lapse[subject]) <- a + bi*intensity + bh*hops + bf*fast + bl*lines, # likelikehood function w/ scaled probit link -->
<!--     threshold[subject] ~ dunif(0,15), # estimate threshold, spread, and lapse parameters of the PF for each subject -->
<!--     spread[subject] ~ dcauchy(0,1), -->
<!--     lapse[subject] ~ dunif(0,.06), -->
<!--     c(a,bi,bh,bf,bl) ~ dnorm(0,10) # weak priors for model coefficients -->
<!--   ), -->
<!--   data = e2df, -->
<!--   chains = 4, -->
<!--   cores = 2, -->
<!--   warmup = 500, -->
<!--   iter = 1500 -->
<!-- ) -->
<!-- ``` -->