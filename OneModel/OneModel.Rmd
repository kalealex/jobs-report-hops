---
title: "One Model To Rule Them All"
author: "Alex Kale"
date: "9/24/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# library(rethinking)
library(rstan)
```

## Experiment 2

### Load Raw Data

These data were collected in a two-alternative forced choice experiement where participants judged stimuli at varying levels of evidence and were either correct or incorrect on each trial. In experiment 2, each participant was assigned to one of three possible uncertainty visualization conditions: regular HOPs, fast HOPs, or line ensembles.

```{r}
# e1df = read.csv("E1-AnonymousRawData-InferenceSample.csv")
e2df = read.csv("E2-AnonymousRawData.csv")
```

### Preprocessing

Reformat the raw data from the experiment as a list, changing types to integers as needed and taking the absolute value of ratio (i.e., stimulus intensity). This step will enable Stan to correctly process the data.

```{r}
# format data for model
e2data <- list(
  N = length(e2df$Ratio),
  S = length(unique(e2df$WorkerID)),
  correct = as.integer(e2df$Correct),
  intensity = abs(e2df$Ratio),
  hops = as.integer(e2df$Visualization=="h"),
  fast = as.integer(e2df$Visualization=="hf"),
  lines = as.integer(e2df$Visualization=="c"),
  subject = as.integer(as.factor(e2df$WorkerID))
)
```

### Model Specification

Our statistical model is a variation on *Bayesian hierarchical probit regression*.

We model the probability of a correct response from 0.5 (guessing) to 1 - lapse rate (the maximum estimated performance for each participant) as a function of stimulus intensity using a _scaled and shifted cumulative normal distribution_. This is called the *psychometric function*, and it will be used as the inverse link function in our regression model. Here are the parameters of the psychometric function:

* threshold: mean of the cumulative Gaussian in stimulus intensity units,
* spread: standard deviation (sd) of the cumulative Gaussian in stimulus intensity units,
* guess rate: the lower bound of the cumulative probability density which is .5 for a two alternative forced choice task with equally likely alternatives
* lapse rate: the rate of error for an individual due to lapses of attention which are independent of the stimulus

Below is the specification of the statistical model. Note the following features of the model:

* We are modelling correctness as a binomial distributed outcome with the probability of being correct as a transformed linear model of ratio and visualization condition.
* We use a _scaled and shifted probit_ (the inverse of the psychometric function) as the link function for our linear model.
* The threshold, spread, and lapse rate parameters of the model are estimated for each subject while guess rate is assumed to be .5 based on the design of the experiment.
* We estimate the effect of each visualization condition via dummy coding and the effect of stimulus intensity (ratio).

The point of this exercise is that all parameters, both at the subject level (psychometric function parameters) and the group level (effects of visualization condition and stimulus intensity), are all estimated from raw data within one unified model.

```{r}
# model specification in Stan
stanCode <- 
'data {
  int<lower=0> N; // number of obs
  int<lower=0> S; // number of subjects
  int<lower=0,upper=1> correct[N]; // correctness of judgments on each trial
  real<lower=0> intensity[N]; // stimulus intensity on each trial
  int<lower=0,upper=1> hops[N]; // dummy code for regular speed HOPs
  int<lower=0,upper=1> fast[N]; // dummy code for fast HOPs
  int<lower=0,upper=1> lines[N]; // dummy code for line ensembles
  int<lower=0> subject[N]; //subject index for each observation
}
parameters {
  real<lower=0> threshold[S]; // estimate threshold, spread, and lapse parameters of the PF for each subject
  real<lower=0> spread[S];
  real<lower=0,upper=0.2> lapse[S];
  real bi; // model coefficients
  real bh;
  real bf;
  real bl;
//  real<lower=0> mu; // hyperparameters for population JNDs
//  real<lower=0> sigma;
}
model {
  vector[N] p; // probability of being correct on each trial
  threshold ~ normal(0,2); // priors for PF parameters
//  threshold ~ normal(mu,sigma); // priors for PF parameters
  spread ~ cauchy(0,2);
  lapse ~ beta(0.026,0.500); // mean = 0.05; 95% interval = 0.1
  bi ~ normal(0,2); // mildly informative priors for model coefficients
  bh ~ normal(0,2);
  bf ~ normal(0,2);
  bl ~ normal(0,2);
//  mu ~ normal(0,5); // weak priors for threshold
//  sigma ~ cauchy(0,2);
  for (i in 1:N) {
    // linear model for effects on perceived stimulus intensity
    real lm;
    lm = bi*intensity[i] + bh*hops[i] + bf*fast[i] + bl*lines[i];
    // psychometric function as inverse link function
    p[i] = (0.5 - lapse[subject[i]])*Phi((lm - threshold[subject[i]])/spread[subject[i]]/sqrt(2.0)) + 0.5;
  }
  correct ~ binomial(1,p);
}'
```

Next we fit the model to our data using Stan.

```{r}
# run model 
e2m <- stan(
  model_code = stanCode,
  data = e2data,
  chains = 2,
  cores = 2,
  warmup = 500,
  iter = 1500,
  control = list(adapt_delta = 0.99,max_treedepth = 15)
)
# e2m <- stan(
#   model_code = stanCode,
#   data = e2data,
#   chains = 2,
#   cores = 2,
#   warmup = 300,
#   iter = 1000
# )
```


### Check Model Perfomance

This kind of Bayesian model samples from a Markov Chain Monte Claro algoritm to estimate each parameter in our model. Here we can see whether the four separate simulations we ran above converged (a sign of reliable estimates) and whether the any the samples from any simulation contain autocorrelation (a sign that out simulation got stuck at a given location in parameter space, and we should run the model fit again and possibly change our model specification or priors). We check this by looking at a traceplot.

```{r}
traceplot(e2m,pars=c('bi','bh','bf','bl'))
```

Here, we just want to see that the simulation is not getting stuck on particular parameter values.

Let's also check whether our model had any problems identifying any of our parameters due to highly correltated parameter values. We check this using a pairs plot.

```{r}
pairs(e2m,pars=c('bi','bh','bf','bl'))
```

Too much correlation between posterior samples would here would indicate that our model fit is probably not properly estimating these parameters due to non-identifiability.

### Results

Let's take a look at our parameter estimates.

```{r}
plot(e2m,pars=c('bi','bh','bf','bl'))
```

We can see here that the point estimates follow the pattern we would expect based on the results of the frequentist model we report in the paper.

* bi: Increasing stimulus intensity increases the perceived level of evidence. This parameter is a good sanity check and it orients us to the meaning of the units for these model coefficients.
* bh: Regular speed HOPs increase the perceived level of evidence. This is consistent with reduced JNDs.
* bf: Fast HOPs seem to have a neutral impact on the perceived level of evidence.
* bl: Line ensembles seem to reduce the perceived level of evidence.

Note that the credibility intervals (CIs) on these estimates are wide. We should expect that a Bayesian hierarchical model, designed to preserve uncertainty in PF fits at the level of estimating population parameters, will be more conservative and have wider CIs.

<!-- ### OLD Preprocessing -->

<!-- ```{r} -->
<!-- # format data for model -->
<!-- e2df$correct <- as.integer(e2df$Correct) -->
<!-- e2df$intensity <- abs(e2df$Ratio) -->
<!-- e2df$hops <- as.integer(e2df$Visualization=="h") -->
<!-- e2df$fast <- as.integer(e2df$Visualization=="hf") -->
<!-- e2df$lines <- as.integer(e2df$Visualization=="c") -->

<!-- # make sure stan doesn't read our data wrong (e.g., in case there are missing subject ids) -->
<!-- e2df$subject <- as.integer(as.factor(e2df$WorkerID)) -->
<!-- ``` -->

<!-- ### OLD Model Specification -->

<!-- Here we model the probability of a correct response from 0.5 (guessing) to 1 - lapse rate (the maximum estimated performance for each participant) as a function of stimulus intensity using a cumulative Gaussian distribution. This is called the psychometric function. We define the psychometric function and its inverse (i.e., a scaled probit) below. Note that the parameters of the psychometric function are defined in these functions: -->

<!-- * threshold: mean of the cumulative Gaussian in stimulus intensity units, -->
<!-- * spread: standard deviation (sd) of the cumulative Gaussian in stimulus intensity units, -->
<!-- * guess rate: the lower bound of the cumulative probability density which is .5 for a two alternative forced choice task with equally likely alternatives -->
<!-- * lapse rate: the rate of error for an individual due to lapses of attention which are independent of the stimulus -->

<!-- ```{r} -->
<!-- # define the psychometric function (PF) and inverse as a rescaled probit, adding parameters for mean, sd, guess rate, and lapse rate of the PF -->
<!-- scaledProbit <- function(p,m,sd,guess,lapse){ -->
<!--   # handle arguments -->
<!--   if(missing(guess)) { -->
<!--     guess <- .5 -->
<!--   } -->
<!--   if( missing(lapse)) { -->
<!--     lapse <- 0 -->
<!--   } -->
<!--   p <- as.matrix(p) -->

<!--   # set upper and lower bounds -->
<!--   lowerBound <- guess -->
<!--   upperBound <- 1 - lapse -->

<!--   # calculate z scores for each value of p -->
<!--   z <- sqrt(2) * qnorm((p - lowerBound) / (upperBound - lowerBound)) -->

<!--   # convert z scores to units of intensity -->
<!--   intensity <- z * sd + m -->
<!--   # place lower bound for stimulus intensity at zero -->
<!--   intensity[is.infinite(intensity)] <- 0 -->
<!--   return(intensity) -->
<!-- } -->
<!-- psychometric <- function(intensity,m,sd,guess,lapse){ -->
<!--   # handle arguments -->
<!--   if(missing(guess)) { -->
<!--     guess <- .5 -->
<!--   } -->
<!--   if( missing(lapse)) { -->
<!--     lapse <- 0 -->
<!--   } -->
<!--   intensity <- as.matrix(intensity) -->

<!--   # set upper and lower bounds -->
<!--   lowerBound <- guess -->
<!--   upperBound <- 1 - lapse -->

<!--   # calculate z score for each level of intensity -->
<!--   r <- dim(intensity)[1] -->
<!--   c <- dim(intensity)[2] -->
<!--   z <- (intensity - m * matrix(1, r, c)) / sd -->

<!--   # scale cumulative probability density between upper and lower bound -->
<!--   p <- lowerBound + pnorm(z / sqrt(2)) * (upperBound - lowerBound) -->
<!--   return(p) -->
<!-- } -->
<!-- ``` -->


<!-- Next, we use the scaled probit as our link function in a binomial regression. Here is the specification of the statistical model. Note the following features of the model: -->

<!-- * We are modelling correctness as a binomial distributed outcome with the probability of being correct as a transformed linear model of ratio and visualization condition. -->
<!-- * We use our scaled probit function (defined above) as the link function for our linear model. -->
<!-- * The threshold, spread, and lapse rate parameters of the model are estimated for each subject while guess rate is assumed to be .5 based on the design of the experiment. -->
<!-- * We estimate the effect of each visualization condition via dummy coding and the effect of stimulus intensity (ratio). -->

<!-- The point of this exercise is that all parameters, both at the subject level (psychometric function parameters) and the group level (effects of visualization condition and stimulus intensity), are all estimated from raw data within one unified model. -->

<!-- ```{r} -->
<!-- # from Richard McElreath's Statistical Rethinking (http://xcelab.net/rm/statistical-rethinking/) -->
<!-- e2m <- map2stan( -->
<!--   alist( -->
<!--     correct ~ dbinom(1,p), # binomial distribution of correctness on each trial -->
<!--     scaledProbit(p,threshold[subject],spread[subject],0.5,lapse[subject]) <- a + bi*intensity + bh*hops + bf*fast + bl*lines, # likelikehood function w/ scaled probit link -->
<!--     threshold[subject] ~ dunif(0,15), # estimate threshold, spread, and lapse parameters of the PF for each subject -->
<!--     spread[subject] ~ dcauchy(0,1), -->
<!--     lapse[subject] ~ dunif(0,.06), -->
<!--     c(a,bi,bh,bf,bl) ~ dnorm(0,10) # weak priors for model coefficients -->
<!--   ), -->
<!--   data = e2df, -->
<!--   chains = 4, -->
<!--   cores = 2, -->
<!--   warmup = 500, -->
<!--   iter = 1500 -->
<!-- ) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- # simpler logistic regression model in Stan -->
<!-- stanCode <- -->
<!-- 'data { -->
<!--   int<lower=0> N; // number of obs -->
<!--   int<lower=0> S; // number of subjects -->
<!--   int<lower=0,upper=1> correct[N]; // correctness of judgments on each trial -->
<!--   real<lower=0> intensity[N]; // stimulus intensity on each trial -->
<!--   int<lower=0> subject[N]; //subject index for each observation -->
<!-- } -->
<!-- parameters { -->
<!--   real a; // model coefficients -->
<!--   real bi; -->
<!--   vector[S] subj; -->
<!--   real<lower=0> sigma; -->
<!-- } -->
<!-- model { -->
<!--   vector[N] p; // probability of being correct on each trial -->
<!--   a ~ normal(0,10); //weak priors for model coefficients -->
<!--   bi ~ normal(0,10); -->
<!--   subj ~ normal(0,sigma); -->
<!--   sigma ~ cauchy(0,1); -->
<!--   for (i in 1:N) { -->
<!--     // scaled and shifted probit link function specific to each subject -->
<!--     p[i] = inv_logit(a + bi*intensity[i] + subj[subject[i]]); -->
<!--   } -->
<!--   correct ~ binomial(1,p); -->
<!-- }' -->
<!-- ``` -->